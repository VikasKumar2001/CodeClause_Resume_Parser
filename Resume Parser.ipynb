{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564550e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser\n",
    "import os\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddbbf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vikas Kumar\n",
      "§ VikasKumar2001 | (cid:239) Vikas Kumar | # kvikas1482@gmail.com | H +91 9628286911\n",
      "\n",
      "Work Experience\n",
      "\n",
      "Aug 2022 - Aug 2023\n",
      "Web Developer\n",
      "As a key member of the website development team, I was responsible for implementing the design and\n",
      "user interface of the mycine club website.\n",
      "Jan 2023 - Feb 2023\n",
      "Mentor at Training Camp 1.0\n",
      "I was responsible for educating students on the fundamental algorithms and applications of Machine\n",
      "Learning.\n",
      "Intern at CodeClause\n",
      "March 2023 - March 2023\n",
      "Completed internship at CodeClause where I gained practical experience in DataScience by creating\n",
      "Machine Learning model.\n",
      "Intern at Innomatics Research Lab\n",
      "Persuing internship at Innomatics Research Lab where I learning about flask, Streamlit and Chatbots,\n",
      "\n",
      "March 2022\n",
      "\n",
      "Projects\n",
      "\n",
      "Movie-Recommendation-System (ML)\n",
      "Link to Code\n",
      "The goal of a movie recommendation system is to personalize the movie-watching experience for each user\n",
      "by suggesting films that are most relevant to their individual tastes.\n",
      "Link to Code\n",
      "Three Layer Encryption For Enhanced E-Commerce Website Security\n",
      "The goal of Three Layer Encryption is to secure the E-commerce website more secure. Review paper in\n",
      "PnrJournal 2023\n",
      "Link to Demo\n",
      "Virtual Car Showroom\n",
      "A virtual car showroom typically includes features such as 3D models of cars, 360-degree views, detailed\n",
      "specifications, and interactive features such as test drives and configurators .\n",
      "\n",
      "Education\n",
      "\n",
      "2021 - 2025 Bachelor’s Degree at KIET GROUP OF INSTITUTION\n",
      "2020\n",
      "\n",
      "Class 12th Ajmani International School\n",
      "\n",
      "(GPA: 7.56)\n",
      "(80)\n",
      "\n",
      "Publications\n",
      "\n",
      "Vikas, Kumar (Jan. 2023). “Paper: Invo-Substitute: Three Layer Encryption For Enhanced E-Commerce\n",
      "Website Security Using Substitution Cipher And Involution Function”. In: Journal of Pharmaceutical\n",
      "Negative Results, 1621–1640. VOL. 14 SPECIAL ISSUE 02 (2023).18, p. 20. url: https : / / www .\n",
      "pnrjournal.com/index.php/home/article/view/7295.\n",
      "\n",
      "Skills\n",
      "\n",
      "Skills\n",
      "Some More Skills LeaderShip , Teaching , Public Speaking\n",
      "\n",
      "C, C++, Python, Machine Learning, Unity3D, AR/VR, HTML5, CSS3, ANN.\n",
      "\n",
      "ACHIEVEMENTS\n",
      "\n",
      "• Hack This November (Inter College Hackathon) Winner.\n",
      "\n",
      "• Top 1.0% performer in MLH Global Hack Week\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    " \n",
    " \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('autoCV (3).pdf'))  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c52156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kvikas1482@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_text\n",
    " EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    " \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    " \n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    " if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('autoCV (3).pdf')\n",
    "    emails = extract_emails(text)\n",
    " \n",
    "    if emails:\n",
    "        print(emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5107d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in e:\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be06a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d05ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VIKAS\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python', 'Machine Learning'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    " \n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    " \n",
    " \n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    " \n",
    " \n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    found_skills = set()\n",
    "\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_docx(\"autoCV (3).docx\")\n",
    "    skills = extract_skills(text)\n",
    " \n",
    "    print(skills) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bd2a3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeaderShip', 'MLH Global Hack Week', 'Inter College Hackathon', 'CSS3', 'HTML5', 'ANN', 'Involution Function', 'CodeClause', 'KIET', 'Pharmaceutical Negative Results', 'VikasKumar2001', 'SPECIAL', 'Training', 'GPA', 'Innomatics Research Lab', 'PnrJournal'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VIKAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VIKAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VIKAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\VIKAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\VIKAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    " \n",
    " \n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "]\n",
    " \n",
    " \n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    " \n",
    " \n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    " \n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    " \n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word):\n",
    "                education.add(org)\n",
    " \n",
    "    return education\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_docx(\"autoCV (3).docx\")\n",
    "    education_information = extract_education(text)\n",
    " \n",
    "    print(education_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf0c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
